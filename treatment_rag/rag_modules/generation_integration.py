"""
ç”Ÿæˆé›†æˆæ¨¡å—
"""

import os
import logging
from typing import List

from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.chat_models.moonshot import MoonshotChat
from langchain_deepseek import ChatDeepSeek
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

logger = logging.getLogger(__name__)

class GenerationIntegrationModule:
    """ç”Ÿæˆé›†æˆæ¨¡å— - è´Ÿè´£LLMé›†æˆå’Œå›ç­”ç”Ÿæˆ"""
    
    def __init__(self, model_name: str = "kimi-k2-0711-preview", temperature: float = 0.1, max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—
        
        Args:
            model_name: æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = None
        self.setup_llm()
    
    def setup_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–LLM: {self.model_name}")

        api_key = "sk-4f4fdb5581e045bc9426add277a90735"
        if not api_key:
            raise ValueError("è¯·è®¾ç½® MOONSHOT_API_KEY ç¯å¢ƒå˜é‡")

        self.llm = ChatDeepSeek(
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            api_key=api_key
        )
        
        logger.info("LLMåˆå§‹åŒ–å®Œæˆ")
    
    def generate_basic_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”ŸæˆåŸºç¡€å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„éª¨ç§‘ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç–¾ç—…ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ²»ç–—æ–¹æ¡ˆ:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        # ä½¿ç”¨LCELæ„å»ºé“¾
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def generate_step_by_step_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ†æ­¥éª¤å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ†æ­¥éª¤çš„è¯¦ç»†å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è„ŠæŸ±å¤–ç§‘ä¸åº·å¤åŒ»å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»å­¦çŸ¥è¯†åº“å†…å®¹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ç–¾ç—…è§£æä¸æ²»ç–—å»ºè®®ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³åŒ»å­¦èƒŒæ™¯/çŸ¥è¯†å›¾è°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ“‹ å»ºè®®æ²»ç–—æ–¹æ¡ˆ
[åŸºäºçŸ¥è¯†åº“ï¼Œåˆ—å‡ºåˆ†é˜¶æ®µçš„æ²»ç–—å»ºè®®ï¼Œå¦‚ï¼šä¿å®ˆæ²»ç–—ï¼ˆè¯ç‰©ã€ç‰©ç†ï¼‰ã€å¾®åˆ›å¹²é¢„æˆ–æ‰‹æœ¯æ–¹æ¡ˆ]

## ğŸ§˜â€â™€ï¸ åº·å¤æŒ‡å¯¼ä¸é”»ç‚¼
[è¯¦ç»†çš„æ“ä½œè¯´æ˜ï¼ŒåŒ…å«å…·ä½“çš„åŠ¨ä½œåç§°ã€é¢‘ç‡ã€æŒç»­æ—¶é—´ä»¥åŠç¦å¿Œäº‹é¡¹ã€‚å¦‚åŸæ–‡åŒ…å«â€œåº·å¤åŠ¨ä½œâ€ï¼Œè¯·åŠ¡å¿…è¯¦ç»†ç½—åˆ—]

## âš ï¸ ä¸“å®¶æé†’
[ä»…åœ¨æœ‰å…³é”®é£é™©ç‚¹æˆ–ç”Ÿæ´»æ³¨æ„äº‹é¡¹æ—¶åŒ…å«ã€‚ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä¸­çš„é£é™©æç¤ºã€‚å¦‚æœæ²¡æœ‰é¢å¤–çš„æ³¨æ„äº‹é¡¹ï¼Œå¯ä»¥åŸºäºä¸´åºŠç»éªŒæ€»ç»“å…³é”®è¦ç‚¹ï¼Œä¾‹å¦‚â€œä½•æ—¶éœ€è¦ç«‹å³å°±åŒ»â€æˆ–â€œç”Ÿæ´»å§¿åŠ¿çŸ«æ­£â€ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- ä¿æŒåŒ»å­¦æœ¯è¯­çš„ä¸“ä¸šæ€§ï¼ŒåŒæ—¶ç¡®ä¿æ™®é€šç”¨æˆ·æ˜“äºç†è§£ã€‚
- ä¸¥ç¦å¼ºè¡Œæä¾›çŸ¥è¯†åº“ä¸­æœªæåŠçš„åŒ»ç–—è¯Šæ–­å»ºè®®ã€‚
- é‡ç‚¹çªå‡ºæ–¹æ¡ˆçš„å®‰å…¨æ€§ä¸å¯æ“ä½œæ€§ï¼ˆä¾‹å¦‚ï¼šæ˜ç¡®æ ‡æ³¨â€œè¯·åœ¨ä¸“ä¸šäººå‘˜æŒ‡å¯¼ä¸‹è¿›è¡Œâ€ï¼‰ã€‚
- å¦‚æœæ²¡æœ‰å…·ä½“çš„åº·å¤åŠ¨ä½œæˆ–æ³¨æ„äº‹é¡¹ï¼Œå¯ä»¥çœç•¥ç›¸åº”éƒ¨åˆ†ã€‚

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response

    def query_rewrite(self, query: str) -> str:
        """
        æ™ºèƒ½æŸ¥è¯¢é‡å†™ - è®©å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            é‡å†™åçš„æŸ¥è¯¢æˆ–åŸæŸ¥è¯¢
        """
        prompt = PromptTemplate(
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·å…³äºè„ŠæŸ±å¥åº·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ï¼Œä»¥ä¼˜åŒ–åœ¨åŒ»å­¦çŸ¥è¯†åº“ä¸­çš„æ£€ç´¢æ•ˆæœã€‚

åŸå§‹æŸ¥è¯¢: {query}

åˆ†æè§„åˆ™ï¼š
1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢**ï¼ˆç›´æ¥è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
   - åŒ…å«å…·ä½“ç–¾ç—…åç§°æˆ–è§£å‰–ä½ï¼šå¦‚"è…°æ¤é—´ç›˜çªå‡ºæ€ä¹ˆæ²»ç–—"ã€"é¢ˆæ¤C4-C5èŠ‚æ®µçªå‡º"
   - æ˜ç¡®çš„ç—‡çŠ¶æè¿°ï¼šå¦‚"ä¸‹è‚¢æ”¾å°„æ€§éº»æœ¨çš„åŸå› "ã€"è…°æ¤æœ¯åä¼¤å£ç–¼ç—›"
   - å…·ä½“çš„æ£€æŸ¥/æœ¯è¯­è¯¢é—®ï¼šå¦‚"æ ¸ç£å…±æŒ¯MRIå¦‚ä½•çœ‹è„±å‡º"ã€"è…°æ¤èåˆæœ¯çš„ç¦å¿Œç—‡"

2. **æ¨¡ç³Šä¸æ¸…æˆ–è¿‡äºå£è¯­åŒ–çš„æŸ¥è¯¢**ï¼ˆéœ€è¦é‡å†™ï¼‰ï¼š
   - è¿‡äºå®½æ³›ï¼šå¦‚"è…°ç—›"ã€"è„–å­éš¾å—"ã€"è„ŠæŸ±æœ‰é—®é¢˜"
   - ç¼ºä¹ä¸´åºŠä¿¡æ¯ï¼šå¦‚"æ¨èä¸ªè¯"ã€"æ€ä¹ˆé”»ç‚¼"ã€"è¯¥çœ‹å“ªä¸ªç§‘"
   - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"è…°å¿«æ–­äº†æ€ä¹ˆåŠ"ã€"è„–å­è½¬ä¸åŠ¨äº†"

é‡å†™åŸåˆ™ï¼š
- **æœ¯è¯­åŒ–**ï¼šå°†å£è¯­è½¬æ¢ä¸ºè§„èŒƒçš„åŒ»å­¦æè¿°ï¼ˆå¦‚"è„–å­éš¾å—" â†’ "é¢ˆæ¤ä¸é€‚æ„Ÿ"ï¼‰ã€‚
- **å…·è±¡åŒ–**ï¼šå¢åŠ â€œç—…å› åˆ†æâ€ã€â€œæ²»ç–—æ–¹æ¡ˆâ€æˆ–â€œåº·å¤é”»ç‚¼â€ç­‰å¼•å¯¼è¯ã€‚
- **ä¿æŒåŸæ„**ï¼šä¸¥ç¦æ”¹å˜ç”¨æˆ·æè¿°çš„éƒ¨ä½æˆ–ç—‡çŠ¶æ€§è´¨ã€‚
- **ç®€æ´æ€§**ï¼šé‡å†™åçš„çŸ­è¯­åº”åˆ©äºæ£€ç´¢ã€‚

ç¤ºä¾‹ï¼š
- "è…°ç—›" â†’ "è…°ç—›çš„å¸¸è§ç—…å› ä¸æ²»ç–—å»ºè®®"
- "è„–å­éš¾å—" â†’ "é¢ˆæ¤ä¸é€‚çš„ç¼“è§£æ–¹æ³•ä¸åº·å¤é”»ç‚¼"
- "æ¨èä¸ªè¯" â†’ "è„ŠæŸ±ç›¸å…³ç–¾ç—…çš„å¸¸ç”¨è¯ç‰©æŒ‡å¯¼"
- "è…°å¿«æ–­äº†" â†’ "æ€¥æ€§è…°éƒ¨å‰§çƒˆç–¼ç—›çš„å¤„ç†æ–¹æ¡ˆ"
- "è…°æ¤é—´ç›˜çªå‡ºæ€ä¹ˆæ²»" â†’ "è…°æ¤é—´ç›˜çªå‡ºæ€ä¹ˆæ²»"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
- "é¢ˆæ¤ç—…åƒä»€ä¹ˆè¯" â†’ "é¢ˆæ¤ç—…åƒä»€ä¹ˆè¯"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

è¯·è¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰:""",
            input_variables=["query"]
        )

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query).strip()

        # è®°å½•é‡å†™ç»“æœ
        if response != query:
            logger.info(f"æŸ¥è¯¢å·²é‡å†™: '{query}' â†’ '{response}'")
        else:
            logger.info(f"æŸ¥è¯¢æ— éœ€é‡å†™: '{query}'")

        return response



    def query_router(self, query: str) -> str:
        """
        æŸ¥è¯¢è·¯ç”± - æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è·¯ç”±ç±»å‹ ('list', 'detail', 'general')
        """
        prompt = ChatPromptTemplate.from_template("""
æ ¹æ®ç”¨æˆ·å…³äºè„ŠæŸ±å¥åº·çš„é—®é¢˜ï¼Œå°†å…¶å‡†ç¡®åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š

1. 'list' - ç”¨æˆ·æƒ³è¦è·å–å„ç§ç–¾ç—…çš„æ²»ç–—æ–¹æ¡ˆã€ç§‘å®¤æ¨èæˆ–è¯å“ã€‚
   ä¾‹å¦‚ï¼šè…°æ¤é—´ç›˜çªå‡ºåº”è¯¥æ€ä¹ˆæ²»ç–—ã€æ¨èå‡ ç§ç¼“è§£é¢ˆæ¤ç—›çš„è†è¯ã€‚

2. 'detail' - ç”¨æˆ·è¯¢é—®å…·ä½“çš„æ²»ç–—æ“ä½œã€åº·å¤é”»ç‚¼æ­¥éª¤ã€æ‰‹æœ¯ç»†èŠ‚æˆ–ç”¨è¯æŒ‡å¯¼ã€‚
   ä¾‹å¦‚ï¼šå°ç‡•é£æ€ä¹ˆåšã€è…°æ¤å¾®åˆ›æ‰‹æœ¯çš„è¿‡ç¨‹æ˜¯æ€æ ·çš„ã€è¿™ç§è¯ä¸€å¤©åƒå‡ æ¬¡ã€æœ¯åå¦‚ä½•ç¿»èº«ã€‚

3. 'general' - ç”¨æˆ·è¯¢é—®ç–¾ç—…çš„å®šä¹‰ã€å‘ç—…åŸç†ã€æ£€æŸ¥æŠ¥å‘Šè§£è¯»æˆ–é¢„é˜²å¸¸è¯†ã€‚
   ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ¤ç®¡ç‹­çª„ã€æ ¸ç£å…±æŒ¯ç»“æœæ€ä¹ˆçœ‹ã€ä¹…åä¸ºä»€ä¹ˆä¼šå¯¼è‡´è…°ç—›ã€é¢ˆæ¤ç—…çš„å±å®³ã€‚

è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼šlistã€detail æˆ– general

ç”¨æˆ·é—®é¢˜: {query}

åˆ†ç±»ç»“æœ:""")

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        result = chain.invoke(query).strip().lower()

        # ç¡®ä¿è¿”å›æœ‰æ•ˆçš„è·¯ç”±ç±»å‹
        if result in ['list', 'detail', 'general']:
            return result
        else:
            return 'general'  # é»˜è®¤ç±»å‹

    def generate_list_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ—è¡¨å¼å›ç­” - é€‚ç”¨äºæ¨èç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ—è¡¨å¼å›ç­”
        """
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„èœå“ä¿¡æ¯ã€‚"

        # æå–èœå“åç§°
        dish_names = []
        for doc in context_docs:
            dish_name = doc.metadata.get('case_report_id', 'æœªçŸ¥ç–¾ç—…')
            if dish_name not in dish_names:
                dish_names.append(dish_name)

        # æ„å»ºç®€æ´çš„åˆ—è¡¨å›ç­”
        if len(dish_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{dish_names[0]}"
        elif len(dish_names) <= 3:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ²»ç–—æ–¹æ¡ˆï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names)])
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ²»ç–—æ–¹æ¡ˆï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names[:3])]) + f"\n\nè¿˜æœ‰å…¶ä»– {len(dish_names)-3} é“èœå“å¯ä¾›é€‰æ‹©ã€‚"

    def generate_basic_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”ŸæˆåŸºç¡€å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            ç”Ÿæˆçš„å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„éª¨ç§‘ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç–¾ç—…ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ²»ç–—æ–¹æ¡ˆ:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def generate_step_by_step_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”Ÿæˆè¯¦ç»†æ­¥éª¤å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            è¯¦ç»†æ­¥éª¤å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è„ŠæŸ±å¤–ç§‘ä¸åº·å¤åŒ»å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»å­¦çŸ¥è¯†åº“å†…å®¹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ç–¾ç—…è§£æä¸æ²»ç–—å»ºè®®ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³åŒ»å­¦èƒŒæ™¯/çŸ¥è¯†å›¾è°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ“‹ å»ºè®®æ²»ç–—æ–¹æ¡ˆ
[åŸºäºçŸ¥è¯†åº“ï¼Œåˆ—å‡ºåˆ†é˜¶æ®µçš„æ²»ç–—å»ºè®®ï¼Œå¦‚ï¼šä¿å®ˆæ²»ç–—ï¼ˆè¯ç‰©ã€ç‰©ç†ï¼‰ã€å¾®åˆ›å¹²é¢„æˆ–æ‰‹æœ¯æ–¹æ¡ˆ]

## ğŸ§˜â€â™€ï¸ åº·å¤æŒ‡å¯¼ä¸é”»ç‚¼
[è¯¦ç»†çš„æ“ä½œè¯´æ˜ï¼ŒåŒ…å«å…·ä½“çš„åŠ¨ä½œåç§°ã€é¢‘ç‡ã€æŒç»­æ—¶é—´ä»¥åŠç¦å¿Œäº‹é¡¹ã€‚å¦‚åŸæ–‡åŒ…å«â€œåº·å¤åŠ¨ä½œâ€ï¼Œè¯·åŠ¡å¿…è¯¦ç»†ç½—åˆ—]

## âš ï¸ ä¸“å®¶æé†’
[ä»…åœ¨æœ‰å…³é”®é£é™©ç‚¹æˆ–ç”Ÿæ´»æ³¨æ„äº‹é¡¹æ—¶åŒ…å«ã€‚ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä¸­çš„é£é™©æç¤ºã€‚å¦‚æœæ²¡æœ‰é¢å¤–çš„æ³¨æ„äº‹é¡¹ï¼Œå¯ä»¥åŸºäºä¸´åºŠç»éªŒæ€»ç»“å…³é”®è¦ç‚¹ï¼Œä¾‹å¦‚â€œä½•æ—¶éœ€è¦ç«‹å³å°±åŒ»â€æˆ–â€œç”Ÿæ´»å§¿åŠ¿çŸ«æ­£â€ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- ä¿æŒåŒ»å­¦æœ¯è¯­çš„ä¸“ä¸šæ€§ï¼ŒåŒæ—¶ç¡®ä¿æ™®é€šç”¨æˆ·æ˜“äºç†è§£ã€‚
- ä¸¥ç¦å¼ºè¡Œæä¾›çŸ¥è¯†åº“ä¸­æœªæåŠçš„åŒ»ç–—è¯Šæ–­å»ºè®®ã€‚
- é‡ç‚¹çªå‡ºæ–¹æ¡ˆçš„å®‰å…¨æ€§ä¸å¯æ“ä½œæ€§ï¼ˆä¾‹å¦‚ï¼šæ˜ç¡®æ ‡æ³¨â€œè¯·åœ¨ä¸“ä¸šäººå‘˜æŒ‡å¯¼ä¸‹è¿›è¡Œâ€ï¼‰ã€‚
- å¦‚æœæ²¡æœ‰å…·ä½“çš„åº·å¤åŠ¨ä½œæˆ–æ³¨æ„äº‹é¡¹ï¼Œå¯ä»¥çœç•¥ç›¸åº”éƒ¨åˆ†ã€‚

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def _build_context(self, docs: List[Document], max_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not docs:
            return "æš‚æ— ç›¸å…³ç–¾ç—…ä¿¡æ¯ã€‚"
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(docs, 1):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = f"ã€æ²»ç–—æ–¹æ¡ˆ {i}ã€‘"
            if 'case_report_id' in doc.metadata:
                metadata_info += f" {doc.metadata['case_report_id']}"
            if 'category' in doc.metadata:
                metadata_info += f" | åˆ†ç±»: {doc.metadata['category']}"
            
            # æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{metadata_info}\n{doc.page_content}\n"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n" + "="*50 + "\n".join(context_parts)
